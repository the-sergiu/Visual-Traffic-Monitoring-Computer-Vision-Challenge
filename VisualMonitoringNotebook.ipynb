{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6e1cdc",
   "metadata": {},
   "source": [
    "# Computer Vision, Assignment 2\n",
    "# Craioveanu Sergiu-Ionut, 407 AI\n",
    "\n",
    "Link to DropBox: https://www.dropbox.com/sh/qifay2hqleoande/AABgOJYZ_1di_SKvSGZ2iPAVa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd11b7a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pandoc\n",
    "# !pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3668f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm  # 4.64.1\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbe0db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9373cede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dir = \"results/\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "task1_dir_result = \"results/Task1/\"\n",
    "if not os.path.exists(task1_dir_result):\n",
    "    os.makedirs(task1_dir_result)\n",
    "    \n",
    "task2_dir_result = \"results/Task2/\"\n",
    "if not os.path.exists(task2_dir_result):\n",
    "    os.makedirs(task2_dir_result)\n",
    "    \n",
    "task3_dir_result = \"results/Task3/\"\n",
    "if not os.path.exists(task3_dir_result):\n",
    "    os.makedirs(task3_dir_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46a899",
   "metadata": {},
   "source": [
    "# Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837fde7",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c02fb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_image(input_image, window_name='image', timeout=0):\n",
    "    \"\"\"\n",
    "    Display an image in a window, resized to 40% of its original dimensions.\n",
    "\n",
    "    Args:\n",
    "        input_image (numpy array): The input image to display.\n",
    "        window_name (str, optional): The name of the window displaying the image. Defaults to 'image'.\n",
    "        timeout (int, optional): The number of milliseconds to wait before closing the window. Defaults to 0.\n",
    "\n",
    "    Note:\n",
    "        If the timeout is set to 0, the window will not close automatically.\n",
    "    \"\"\"\n",
    "    resized_image = cv.resize(\n",
    "        input_image,\n",
    "        (int(input_image.shape[1] * 0.4), int(input_image.shape[0] * 0.4))\n",
    "    )\n",
    "\n",
    "    cv.imshow(window_name, resized_image)\n",
    "    cv.waitKey(timeout)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61382915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_jpg_files_in_directory(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads all .jpg or .jpeg files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory where files should be read from.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list containing the filenames of all .jpg or .jpeg files in the directory.\n",
    "    \"\"\"\n",
    "    jpg_files = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "            jpg_files.append(filename)\n",
    "    return jpg_files\n",
    "\n",
    "def read_mp4_files_in_directory(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads all .mp4 files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory where files should be read from.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list containing the filenames of all .mp4 files in the directory.\n",
    "    \"\"\"\n",
    "    mp4_files = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.mp4'):\n",
    "            mp4_files.append(filename)\n",
    "    return mp4_files\n",
    "\n",
    "def get_key(dictionary: dict, val: Union[int, str, float, list, dict]) -> Union[int, str, None]:\n",
    "    \"\"\"\n",
    "    Get the key for a given value in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary to find the key in.\n",
    "        val (Union[int, str, float, list, dict]): The value for which the key should be found.\n",
    "\n",
    "    Returns:\n",
    "        Union[int, str, None]: The key associated with the provided value in the dictionary. Returns None if not found.\n",
    "    \"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == val:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf999d6b",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59d0d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    Draw lines on an image given a list of lines.\n",
    "\n",
    "    Args:\n",
    "        img (numpy array): The image on which lines should be drawn.\n",
    "        lines (list): List of lines where each line is a list of four integers [x1, y1, x2, y2] representing the two endpoints of the line.\n",
    "        color (list, optional): List of three integers representing RGB color of the line. Defaults to [255, 0, 0] (Red).\n",
    "        thickness (int, optional): Thickness of the line. Defaults to 2.\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def select_points(event, x, y, flags, param):\n",
    "    \"\"\"\n",
    "    If left button down event is detected, draw a circle at the mouse location and append its position to a list.\n",
    "\n",
    "    Args:\n",
    "        event (int): OpenCV event type.\n",
    "        x (int): x-coordinate of the event.\n",
    "        y (int): y-coordinate of the event.\n",
    "        flags (int): Any relevant flags passed by OpenCV.\n",
    "        param: Any extra parameters supplied by OpenCV.\n",
    "    \"\"\"\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        cv2.circle(image, (x,y), 5, (255,0,0), -1)\n",
    "        points.append((x, y))\n",
    "        print(f\"Point selected: ({x}, {y})\")\n",
    "\n",
    "def get_coord_return_rect(x_min, y_min, x_max, y_max):\n",
    "    \"\"\"\n",
    "    Convert coordinates to rectangle dimensions.\n",
    "\n",
    "    Args:\n",
    "        x_min, y_min, x_max, y_max (int): The minimum and maximum x and y coordinates.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (int, int, int, int): A tuple representing the top-left corner and dimensions (width, height) of the rectangle.\n",
    "    \"\"\"\n",
    "    w = np.abs(x_max - x_min)\n",
    "    h = np.abs(y_max - y_min)\n",
    "    return x_min, y_min, w, h\n",
    "\n",
    "def get_frames_and_rect(file_data: list):\n",
    "    \"\"\"\n",
    "    Extract the number of frames and rectangle from the file data.\n",
    "\n",
    "    Args:\n",
    "        file_data (list): The data extracted from the file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: The number of frames and the rectangle as a tuple (x, y, w, h).\n",
    "    \"\"\"\n",
    "    no_frames = file_data[0][0]\n",
    "    xmin, ymin, xmax, ymax = file_data[1][1:]\n",
    "    x, y, w, h = get_coord_return_rect(xmin, ymin, xmax, ymax)\n",
    "\n",
    "    return no_frames, (x, y, w, h)\n",
    "\n",
    "def get_rect_return_coord(x, y, w, h):\n",
    "    \"\"\"\n",
    "    Convert rectangle dimensions to coordinates.\n",
    "\n",
    "    Args:\n",
    "        x, y, w, h (int): The top-left corner and dimensions (width, height) of the rectangle.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (int, int, int, int): A tuple representing the minimum and maximum x and y coordinates.\n",
    "    \"\"\"\n",
    "    return x, y, x+w, y+h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a2e525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_roi(input_image, rectangle: tuple, display=False):\n",
    "    \"\"\"\n",
    "    Extract the region of interest (ROI) from an image based on the provided rectangle.\n",
    "\n",
    "    Args:\n",
    "        input_image (numpy array): The input image to extract the ROI from.\n",
    "        rectangle (tuple): A tuple containing the coordinates (x, y) and dimensions (w, h) of the ROI.\n",
    "        display (bool): If True, display the extracted ROI using the show_image() function.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The extracted region of interest (ROI) from the input image.\n",
    "    \"\"\"\n",
    "    (x_coord, y_coord, width, height) = rectangle\n",
    "    drawn_image = input_image.copy()\n",
    "    cv2.rectangle(drawn_image, (x_coord, y_coord), (x_coord + width, y_coord + height), (0, 0, 255), 2)  # Red\n",
    "\n",
    "    roi_image = input_image[y_coord:y_coord + height, x_coord:x_coord + width]\n",
    "    if display:\n",
    "        show_image(roi_image)\n",
    "\n",
    "    return roi_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfcb51",
   "metadata": {},
   "source": [
    "## Masking Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752d206e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_mask_given_points(image, points) -> np.array:\n",
    "    \"\"\"Function used for masking entire image except the given polygon\"\"\"\n",
    "    mask = np.zeros_like(image)\n",
    "    cv2.fillPoly(mask, np.array([points]), (255,255,255))\n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81923f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mask_rectangle_except_roi(image, x, y, w, h):\n",
    "    # Create a black image with the same size as the original image\n",
    "    mask = np.zeros_like(image)\n",
    "    \n",
    "    # Set the region of interest (ROI) in the mask to white\n",
    "    mask[y:y+h, x:x+w] = 255\n",
    "\n",
    "    # Perform a bitwise-and operation to mask the original image\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    \n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df2854",
   "metadata": {},
   "source": [
    "# Compute IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83c160a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_iou(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) score of two binary masks.\n",
    "    \n",
    "    Args:\n",
    "        mask1 (numpy array): A binary mask.\n",
    "        mask2 (numpy array): A binary mask.\n",
    "\n",
    "    Returns:\n",
    "        float: The IoU score of the two masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the intersection of the two masks\n",
    "    intersection = np.logical_and(mask1, mask2)\n",
    "    \n",
    "    # Compute the union of the two masks\n",
    "    union = np.logical_or(mask1, mask2)\n",
    "    \n",
    "    # Compute the IoU score\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    \n",
    "    return iou_score\n",
    "\n",
    "\n",
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        bb1 (tuple): A bounding box in the format (x, y, width, height).\n",
    "        bb2 (tuple): A bounding box in the format (x, y, width, height).\n",
    "\n",
    "    Returns:\n",
    "        float: The IoU score of the two bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[0]+bb1[2], bb2[0]+bb2[2])\n",
    "    y_bottom = min(bb1[1]+bb1[3], bb2[1]+bb2[3])\n",
    "\n",
    "    # If there is no overlap, return 0\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute the area of the intersection rectangle\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # Compute the area of both bounding boxes\n",
    "    bb1_area = bb1[2] * bb1[3]\n",
    "    bb2_area = bb2[2] * bb2[3]\n",
    "\n",
    "    # Compute the IoU score\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_max_iou_from_candidates(bounding_box: tuple, candidates: list):\n",
    "    \"\"\"\n",
    "    Compute the maximum IoU score between a given bounding box and a list of candidate bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        bounding_box (tuple): A bounding box in the format (x, y, width, height).\n",
    "        candidates (list): A list of candidate bounding boxes in the format (x, y, width, height).\n",
    "\n",
    "    Returns:\n",
    "        Tuple (float, tuple): The maximum IoU score and the corresponding candidate bounding box.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the maximum IoU score and the best candidate\n",
    "    max_iou = 0.0\n",
    "    best_candidate = None\n",
    "    \n",
    "    # Iterate over all candidate bounding boxes\n",
    "    for candidate in candidates:\n",
    "        # Compute the IoU score for the current candidate\n",
    "        temp_iou = get_iou(candidate, bounding_box)\n",
    "        \n",
    "        # If the current candidate has a higher IoU score, update the maximum IoU score and the best candidate\n",
    "        if temp_iou > max_iou:\n",
    "            max_iou = temp_iou\n",
    "            best_candidate = candidate\n",
    "            \n",
    "    return max_iou, best_candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb59f8",
   "metadata": {},
   "source": [
    "## YoloV7 Setup + Functions \n",
    "\n",
    "It was pretty clear to me, starting with Task 1, that I would be in need of a model which can (somewhat) accurately detect vehicles in images. It was also relevant to me for the model to be good, whilst having little computational overhead. After some digging, I was very pleased to find that `OpenCV` has its own mechanism of importing Deep Learning models, to some extent. Luckily, `YOLO` was among them. `YOLO` is able to detect at least 80 classes of common objects, including cars, trucks, busses or motorbikes. \n",
    "\n",
    "At the beginning, I actually solved all 3 tasks using `YOLOv3`, as it was the best documented and seemingly the most popular model out there. The accuracy obtained was OK on Task1 on the train dataset (92%), but posed somewhat of a problem for task 2 and 3 in some cases where the lighting was poor (i.e. at night). As such, after having all the boilerplate ready, I wondered if there was a better model out there, that would require minimal changes. Apparently, there were newew, better, and faster versions of the YOLO model. At first I opted for `YOLOv4` which was better but considerably slower. I then found `YOLOv7` as one of the better models, moving as fast as `YOLOv3`, but with the accuracy of `YOLOv4` - best of both worlds, if you will.\n",
    "\n",
    "The `YOLO` model requires 3 files:\n",
    "- `yolov7.cfg` -> model configuration file\n",
    "- `yolov7.weights` -> artefact of model weights, essential for our predictions\n",
    "- `coco.names` -> YOLO was trained on the COCO dataset, based on the classes found within those images. It needs this file in order to correctly associate prediction with their class.\n",
    "\n",
    "In the cell below, we'll notice that the setup is relatively quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928aaf87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modelConfiguration = './yolov3.cfg' -> video 6 task 3 greseste 1, cam 12 min\n",
    "# modelWeights = './yolov3.weights'\n",
    "# modelConfiguration = './yolov4-p5.cfg' -> merge mai bine,  video 6 task 3 15 min\n",
    "# modelWeights = './yolov4-p5.weights'\n",
    "modelConfiguration = './yolov7.cfg' # -> merge cam ca v4, dar mai rapid, video 6 task 3 12 min\n",
    "modelWeights = './yolov7.weights'\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Load names of classes from coco\n",
    "classes = None\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get the output layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187924c4",
   "metadata": {},
   "source": [
    "The function below performs \"the magic\" - it will receive an image and return the bounding boxes of all objects that match our classes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc1cb73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def identify_vehicles_in_image(input_image: np.ndarray, display_results: bool = False) -> List[Tuple[int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Identify and draw bounding boxes around vehicles in an image using YOLO object detection.\n",
    "\n",
    "    Args:\n",
    "        input_image (numpy array): A 3D numpy array representing an image where vehicles should be detected.\n",
    "        display_results (bool): A boolean that, if True, displays the image with detected objects bounded.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple represents the rectangle bounding box of a detected vehicle. Each tuple contains 4 integers, \n",
    "                        representing (x, y, width, height) of the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the image to prevent in-place modifications\n",
    "    image_copy = input_image.copy()\n",
    "    image_height, image_width, image_channels = image_copy.shape\n",
    "\n",
    "    # Create a 4D blob from image, with given parameters\n",
    "    blob = cv2.dnn.blobFromImage(image_copy, 0.00392, (1920, 864), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Execute a forward pass through the network to get output from the output layers\n",
    "    output_layers_data = net.forward(output_layers)\n",
    "\n",
    "    detected_classes = []\n",
    "    detection_confidences = []\n",
    "    bounding_boxes = []\n",
    "    classes_of_interest = {'car', 'motorbike', 'bus', 'truck'}\n",
    "\n",
    "    for output_layer in output_layers_data:\n",
    "        for detected_object in output_layer:\n",
    "            scores = detected_object[5:]\n",
    "            class_index = np.argmax(scores)\n",
    "            object_confidence = scores[class_index]\n",
    "\n",
    "            # Filter for classes of interest\n",
    "            if object_confidence > 0.5 and classes[class_index] in classes_of_interest:\n",
    "                # Object has been detected\n",
    "                center_x = int(detected_object[0] * image_width)\n",
    "                center_y = int(detected_object[1] * image_height)\n",
    "                box_width = int(detected_object[2] * image_width)\n",
    "                box_height = int(detected_object[3] * image_height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x_coord = int(center_x - box_width / 2)\n",
    "                y_coord = int(center_y - box_height / 2)\n",
    "\n",
    "                bounding_boxes.append([x_coord, y_coord, box_width, box_height])\n",
    "                detection_confidences.append(float(object_confidence))\n",
    "                detected_classes.append(class_index)\n",
    "\n",
    "    valid_indexes = cv2.dnn.NMSBoxes(bounding_boxes, detection_confidences, 0.5, 0.3)\n",
    "    final_rectangles = []\n",
    "\n",
    "    for i in range(len(bounding_boxes)):\n",
    "        if i in valid_indexes:\n",
    "            x, y, box_width, box_height = bounding_boxes[i]\n",
    "            object_label = str(classes[detected_classes[i]])\n",
    "            cv2.rectangle(image_copy, (x, y), (x + box_width, y + box_height), (0,255,0), 2)\n",
    "            cv2.putText(image_copy, object_label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            final_rectangles.append((x, y, box_width, box_height))\n",
    "    \n",
    "    if display_results:\n",
    "        show_image(image_copy)\n",
    "\n",
    "    return final_rectangles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e44a1",
   "metadata": {},
   "source": [
    "## Task-specific functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9ad15",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59583349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_numbers_task1(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_number = int(file.readline().strip())\n",
    "        rest_of_numbers = [int(line.strip()) for line in file.readlines()]\n",
    "        return first_number, rest_of_numbers\n",
    "\n",
    "def read_numbers_gt(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        numbers = [tuple(map(int, line.strip().split())) for line in lines]\n",
    "        return numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07262b78",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6260e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file_structure_task2(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            numbers = list(map(int, line.strip().split()))\n",
    "            data.append(numbers)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19789a79",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbd73cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_tracker_path(tracker_path: dict) -> dict:\n",
    "    for k, v in tracker_path.items():\n",
    "        tmp_list = [i for i in v if i is not None]\n",
    "        tmp_list = list(dict.fromkeys(tmp_list))\n",
    "        tracker_path[k] = tmp_list\n",
    "    return tracker_path\n",
    "\n",
    "\n",
    "def check_region_task3_bounding_box(image, rectangle):\n",
    "    \n",
    "    reg1 = [(232, 422), (893, 343), (226, 7), (21, 4)]\n",
    "    reg2 = [(1029, 380), (1384, 556), (1902, 430), (1906, 277)]\n",
    "    reg3 = [(438, 700), (1378, 559), (1919, 869), (582, 878)]\n",
    "\n",
    "    # Generate masked image of non-masked car (in bounding box)\n",
    "    unmasked_car = mask_rectangle_except_roi(image, *rectangle)\n",
    "    \n",
    "    max_iou = 0.0\n",
    "    \n",
    "    for reg in [reg1, reg2, reg3]:\n",
    "    \n",
    "        # Generate masked image of non-masked lane\n",
    "        mask_for_reg = generate_mask_given_points(image, reg)\n",
    "\n",
    "        # Compute IoU between non-masked rectangle and non-masked lane area\n",
    "        iou = compute_iou(unmasked_car, mask_for_reg)\n",
    "\n",
    "        if iou > max_iou:\n",
    "            max_iou = iou\n",
    "            reg_pred = reg\n",
    "            \n",
    "    if max_iou > 0.02:\n",
    "        if reg_pred == reg1:\n",
    "            return 1\n",
    "        elif reg_pred == reg2:\n",
    "            return 2\n",
    "        elif reg_pred == reg3:\n",
    "            return 3\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_score_tracker_path(tracker_path: dict, display = True) -> str:\n",
    "    a1_2 = a1_3 = a2_1 = a2_3 = a3_1 = a3_2 = 0\n",
    "    \n",
    "    for k, v in tracker_path.items():\n",
    "        if len(v) == 2:\n",
    "            start = v[0]\n",
    "            end = v[1]\n",
    "            \n",
    "            if start == 1:\n",
    "                if end == 2:\n",
    "                    a1_2 += 1\n",
    "                elif end == 3:\n",
    "                    a1_3 += 1\n",
    "            elif start == 2:\n",
    "                if end == 1:\n",
    "                    a2_1 += 1\n",
    "                elif end == 3:\n",
    "                    a2_3 += 1\n",
    "            elif start == 3:\n",
    "                if end == 1:\n",
    "                    a3_1 += 1\n",
    "                elif end == 2:\n",
    "                    a3_2 += 1\n",
    "                    \n",
    "    output_str = f\"1-2 {a1_2}\\n1-3 {a1_3}\\n2-1 {a2_1}\\n2-3 {a2_3}\\n3-1 {a3_1}\\n3-2 {a3_2}\"\n",
    "    if display:\n",
    "        print(output_str)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d52c0",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a2159",
   "metadata": {},
   "source": [
    "### Generate custom poligons for each lane\n",
    "This cell is used for creating a polygon containing each lane from the image, and masking everything that is not within that polygon (lane). As such, we can generate 9 polygons belonging to the 9 lanes in the camera field of view. We can then predict cars using bounding boxes and perform logical operations using only the masks. Using this method, the car detection operation is performed once per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c962b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # List to store points\n",
    "# points = []\n",
    "\n",
    "# # Load the image\n",
    "# image = task1_imgs[8].copy()\n",
    "# # image = cv2.imread('image.jpg')\n",
    "# cv2.namedWindow('image')\n",
    "# cv2.setMouseCallback('image', select_points)\n",
    "\n",
    "# while(1):\n",
    "#     cv2.imshow('image', image)\n",
    "#     if cv2.waitKey(20) & 0xFF == 27 or len(points) == 4:\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Check if 4 points were selected\n",
    "# if len(points) == 4:\n",
    "#     # Create a mask and fill in the polygon\n",
    "#     mask = np.zeros_like(image)\n",
    "#     cv2.fillPoly(mask, np.array([points]), (255,255,255))\n",
    "\n",
    "#     # Apply the mask to the image\n",
    "#     masked_image = cv2.bitwise_and(image, mask)\n",
    "\n",
    "#     # Show the image\n",
    "#     cv2.imshow('Image', masked_image)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "# else:\n",
    "#     print(\"Four points were not selected\")\n",
    "# print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "710b9398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lanes \"bounding coordinates\" within an image, in order to compute if a car's bounding box is on a specific lane or not\n",
    "lanes = [\n",
    "    [(227, 418), (405, 378), (135, 119), (99, 202)],     # 1\n",
    "    [(405, 395), (555, 379), (108, 39), (65, 52)],       # 2\n",
    "    [(530, 379), (690, 367), (114, 2), (45, 4)],        # 3\n",
    "    [(1114, 350), (1212, 409), (1914, 334), (1914, 276)], # 4\n",
    "    [(1178, 387), (1273, 433), (1913, 358), (1913, 315)], # 5\n",
    "    [(1233, 414), (1321, 467), (1914, 395), (1914, 338)], # 6\n",
    "    [(1189, 621), (1610, 877), (1838, 770), (1443, 580)], # 7\n",
    "    [(1028, 643), (1368, 877), (1734, 873), (1276, 610)], # 8\n",
    "    [(894, 663), (1123, 876), (1474, 875), (1121, 628)],  # 9  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7c22",
   "metadata": {},
   "source": [
    "## Task 1 Approach\n",
    "\n",
    "The approach towards task 1 is somewhat intuitive, in my opinion. We will take all images of interest from Task 1, and for each image, we will predict all the vehicles in that image. Each prediction will output some 'rectangles' coresponding to our bounding boxes of objects. \n",
    "\n",
    "As mentioned earlier, I have created polygons matching the shape of a lane within a given frame. What I then do: for any given image, mask everything (black it out) except a given lane. As such, I am able to single out that lane, check with the bounding boxes generated by `YOLO`, and see how many of my objects overlap with any given lane. The overlap is computed using a modified IoU. I then do this for all lanes in that frame.\n",
    "\n",
    "This cell required some tweaking in the values of IoU, as well as playing with `YOLO`'s confidence paramteres, but on the train set, it outputs an accuracy of 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87da884",
   "metadata": {},
   "source": [
    "### Generate Predictions for Task 1\n",
    "This cell should take at most 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8a3ed04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_1.jpg', '01_2.jpg', '01_3.jpg', '02_1.jpg', '02_2.jpg']\n",
      "['01_1_query.txt', '01_2_query.txt', '01_3_query.txt', '02_1_query.txt', '02_2_query.txt']\n",
      "['01_1_gt.txt', '01_2_gt.txt', '01_3_gt.txt', '02_1_gt.txt', '02_2_gt.txt']\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set data path\n",
    "path = f'{mode}/Task1/'\n",
    "\n",
    "\n",
    "# Store image names\n",
    "image_names = read_jpg_files_in_directory(path)\n",
    "print(image_names[:5])\n",
    "\n",
    "queries = [f\"{img_name.split('.')[0]}_query.txt\" for img_name in image_names]\n",
    "print(queries[:5])\n",
    "\n",
    "gt_files = [f\"{img_name.split('.')[0]}_gt.txt\" for img_name in image_names]\n",
    "print(gt_files[:5])\n",
    "\n",
    "# Store images\n",
    "task1_imgs = [np.array(cv2.imread(os.path.join(path, file))).astype(np.uint8) for file in image_names]\n",
    "\n",
    "for i, img in enumerate(task1_imgs):\n",
    "    \n",
    "    # List of lanes to search\n",
    "    first_number, lanes_to_search = read_numbers_task1(path + queries[i])\n",
    "    \n",
    "    # Generate output\n",
    "    output_str = f\"{first_number}\\n\"\n",
    "    \n",
    "    # Load the image\n",
    "    image = img.copy()\n",
    "    \n",
    "    # Generate car predictions (bounding boxes) over non-masked image\n",
    "    rectangles = identify_vehicles_in_image(image)\n",
    "    \n",
    "    # Iterate through lanes idx (which is offset by 1)\n",
    "    for lane in lanes_to_search:\n",
    "        \n",
    "        # Generate masked image of non-masked lane\n",
    "        mask_for_lane = generate_mask_given_points(image, lanes[lane - 1])\n",
    "        \n",
    "        car_count_lane = 0\n",
    "        # Iterate through car prediction bounding boxes\n",
    "        for rectangle in rectangles:\n",
    "\n",
    "            # Generate masked image of non-masked car (in bounding box)\n",
    "            unmasked_car = mask_rectangle_except_roi(image, *rectangle)\n",
    "\n",
    "            # Compute IoU between non-masked rectangle and non-masked lane area\n",
    "            iou = compute_iou(unmasked_car, mask_for_lane)\n",
    "            \n",
    "            if iou > 0.05:\n",
    "                car_count_lane += 1\n",
    "#                 print(f\"Car detected on lane {j + 1} with IoU {iou}\")\n",
    "        \n",
    "        # Update output string\n",
    "        if car_count_lane > 0:\n",
    "            output_str += f\"{lane} 1\\n\"\n",
    "        else:\n",
    "            output_str += f\"{lane} 0\\n\"\n",
    "#         print(f\"Detected {car_count_lane} cars on lane {j + 1}\\n\")\n",
    "\n",
    "    # Save output string to file\n",
    "    with open(task1_dir_result + image_names[i].split(\".\")[0] + \"_predicted.txt\", 'w') as f:\n",
    "            f.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6537b7",
   "metadata": {},
   "source": [
    "**Train**: Mistakes: 12; Total preds: 154; Score: 0.922077922077922"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d1b2d",
   "metadata": {},
   "source": [
    "### CHECK: Compare gt with preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27e97668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/Task1/\n",
      "test/Task1/ground-truth/\n",
      "IMG NAME: 01_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 01_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 01_3.jpg\n",
      "\n",
      "GT: (4, 0); PRED: (4, 1)\n",
      "--------------------------\n",
      "IMG NAME: 02_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 02_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 02_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 03_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 03_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 03_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 04_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 04_2.jpg\n",
      "\n",
      "GT: (7, 0); PRED: (7, 1)\n",
      "--------------------------\n",
      "IMG NAME: 04_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 05_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 05_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 05_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 06_1.jpg\n",
      "\n",
      "GT: (2, 1); PRED: (2, 0)\n",
      "--------------------------\n",
      "IMG NAME: 06_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 06_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 07_1.jpg\n",
      "\n",
      "GT: (9, 0); PRED: (9, 1)\n",
      "--------------------------\n",
      "IMG NAME: 07_2.jpg\n",
      "\n",
      "GT: (1, 1); PRED: (1, 0)\n",
      "--------------------------\n",
      "IMG NAME: 07_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 08_1.jpg\n",
      "\n",
      "GT: (8, 0); PRED: (8, 1)\n",
      "--------------------------\n",
      "IMG NAME: 08_2.jpg\n",
      "\n",
      "GT: (2, 1); PRED: (2, 0)\n",
      "GT: (6, 0); PRED: (6, 1)\n",
      "--------------------------\n",
      "IMG NAME: 08_3.jpg\n",
      "\n",
      "GT: (7, 0); PRED: (7, 1)\n",
      "--------------------------\n",
      "IMG NAME: 09_1.jpg\n",
      "\n",
      "GT: (9, 0); PRED: (9, 1)\n",
      "--------------------------\n",
      "IMG NAME: 09_2.jpg\n",
      "\n",
      "GT: (9, 0); PRED: (9, 1)\n",
      "--------------------------\n",
      "IMG NAME: 09_3.jpg\n",
      "\n",
      "GT: (3, 0); PRED: (3, 1)\n",
      "--------------------------\n",
      "IMG NAME: 10_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 10_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 10_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 11_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 11_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 11_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 11_4.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 12_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 12_2.jpg\n",
      "\n",
      "GT: (4, 0); PRED: (4, 1)\n",
      "--------------------------\n",
      "IMG NAME: 12_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 12_4.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 13_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 13_2.jpg\n",
      "\n",
      "GT: (9, 0); PRED: (9, 1)\n",
      "--------------------------\n",
      "IMG NAME: 13_3.jpg\n",
      "\n",
      "GT: (3, 0); PRED: (3, 1)\n",
      "--------------------------\n",
      "IMG NAME: 13_4.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 14_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 14_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 14_3.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 14_4.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 15_1.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 15_2.jpg\n",
      "\n",
      "--------------------------\n",
      "IMG NAME: 15_3.jpg\n",
      "\n",
      "GT: (7, 0); PRED: (7, 1)\n",
      "--------------------------\n",
      "IMG NAME: 15_4.jpg\n",
      "\n",
      "GT: (9, 0); PRED: (9, 1)\n",
      "--------------------------\n",
      "Mistakes: 17; Total preds: 150; Score: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "gt_path = path + \"ground-truth/\"\n",
    "print(task1_dir_result)\n",
    "print(gt_path)\n",
    "\n",
    "total_preds = 0\n",
    "mistakes = 0\n",
    "for i, name in enumerate(image_names):\n",
    "    \n",
    "    \n",
    "    print(f\"IMG NAME: {name}\\n\")\n",
    "    # get image name\n",
    "    img_name = name.split(\".\")[0]\n",
    "    \n",
    "    # reconstruct preds file name\n",
    "    pred_file = img_name + \"_predicted.txt\"\n",
    "    \n",
    "    # get the ground truths\n",
    "    gt_list = read_numbers_gt(gt_path + gt_files[i])\n",
    "    \n",
    "    total_preds += gt_list[0][0]\n",
    "    \n",
    "    # get the predictions\n",
    "    pred_list = read_numbers_gt(task1_dir_result + pred_file)\n",
    "    \n",
    "    # count mistakes\n",
    "    for gt, pred in zip(gt_list, pred_list):\n",
    "        if gt != pred:\n",
    "            print(f\"GT: {gt}; PRED: {pred}\")\n",
    "            mistakes += 1\n",
    "    \n",
    "    print(\"--------------------------\")\n",
    "\n",
    "print(f\"Mistakes: {mistakes}; Total preds: {total_preds}; Score: {(total_preds - mistakes) / total_preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf357e",
   "metadata": {},
   "source": [
    "# --- END OF TASK 1 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9855c",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef3b0c",
   "metadata": {},
   "source": [
    "## First approach - just tracker\n",
    "In my first attempt at task 2, I leveraged a `CSRT` object tracker within OpenCV in order to track a car throughout the frames of a video. Given the initial coordinates, I would convert these coordinates to a bounding box, and use it to initialize my tracker. It worked OK in general (say, 10 videos out of 15), but had major issues if a car moved too fast, or if it suddenly overlapped with multiple other cars or objects. I then thought of how I could make my solution more robust, which was actually inspired by task 3. In the next approach, I'll describe the changes brought.\n",
    "\n",
    "I also tried `KCF, GOTURN, MOSSE, TLD` trackers. All failed quite miserably, unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bda2836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# path = f'{mode}/Task2/'\n",
    "# gt_path = path + \"ground-truth/\"\n",
    "\n",
    "# # Store Video names\n",
    "# video_names = read_mp4_files_in_directory(path)\n",
    "# print(video_names[:5])\n",
    "\n",
    "# task2_txt = [f\"{video_name.split('.')[0]}.txt\" for video_name in video_names]\n",
    "# print(task2_txt[:5])\n",
    "\n",
    "# test_names = ['01.mp4', '02.mp4', '05.mp4', '08.mp4']\n",
    "\n",
    "# tracker_types = [ 'MOSSE', 'TLD']\n",
    "\n",
    "# for tracker_type in tracker_types:\n",
    "#     # replace test_names with video_names\n",
    "#     print(f\"================{tracker_type}===============\\n\")\n",
    "#     for j, video_name in enumerate(test_names):\n",
    "        \n",
    "#         print(f'-------{video_name}------\\n')\n",
    "\n",
    "#         txt_file = video_name.split(\".\")[0] + \".txt\"\n",
    "\n",
    "#         # Path to text file\n",
    "#         file_path = path + txt_file\n",
    "\n",
    "#         # Read in the 2 lists\n",
    "#         file_data = read_file_structure_task2(file_path)\n",
    "\n",
    "#         # Get number of frames and bounding box\n",
    "#         no_frames, rect_init = get_frames_and_rect(file_data)\n",
    "#         print(no_frames)\n",
    "\n",
    "#         # Create Output String\n",
    "#         output_str = f\"{no_frames} -1 -1 -1 -1\\n\" \\\n",
    "#                      f\"0 {rect_init[0]} {rect_init[1]} {rect_init[2]} {rect_init[3]}\\n\"\n",
    "\n",
    "#         # Path to your video file\n",
    "#         video_path = path + video_name\n",
    "\n",
    "#         # Create a VideoCapture object\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#         # Check if video opened successfully\n",
    "#         if (cap.isOpened()== False): \n",
    "#             print(\"Error opening video stream or file\")\n",
    "\n",
    "#         # Create a tracker\n",
    "#         if tracker_type == 'KCF':\n",
    "#             tracker = cv2.TrackerKCF_create()\n",
    "#         if tracker_type == 'GOTURN':\n",
    "#             tracker = cv2.TrackerGOTURN_create()\n",
    "#         if tracker_type == 'MOSSE':\n",
    "#             tracker = cv2.TrackerMOSSE_create()\n",
    "#         if tracker_type == 'TLD':\n",
    "#             tracker = cv2.TrackerTLD_create()\n",
    "# #         tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "#         # Get first frame\n",
    "#         ret, frame_init = cap.read()\n",
    "\n",
    "#         cv2.rectangle(\n",
    "#             frame_init, \n",
    "#             (rect_init[0], rect_init[1]), \n",
    "#             (rect_init[0] + rect_init[2], rect_init[1] + rect_init[3]), \n",
    "#             (0, 255, 0), \n",
    "#             2\n",
    "#         )\n",
    "\n",
    "#         # DEBUG: check frame\n",
    "# #         show_image(frame_init)\n",
    "\n",
    "#         # Initialize tracker with first frame and bounding box\n",
    "#         ok = tracker.init(frame_init, rect_init)\n",
    "\n",
    "#         # Read until video is completed\n",
    "#         for i in tqdm(range(1, no_frames)):\n",
    "\n",
    "#             # Read a new frame\n",
    "#             ok, frame = cap.read()\n",
    "#             if not ok:\n",
    "#                 break\n",
    "\n",
    "#             # Update tracker\n",
    "#             ok, bbox = tracker.update(frame)\n",
    "\n",
    "#             # Tracking success\n",
    "# #             p1 = (int(bbox[0]), int(bbox[1]))\n",
    "# #             p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "# #             cv2.rectangle(frame, p1, p2, (0,255, 0), 2, 1)\n",
    "\n",
    "#             # Create coordinates from rectangle and append to output str\n",
    "#             xmin, ymin, xmax, ymax = get_rect_return_coord(*bbox)\n",
    "\n",
    "#             # Append output_str\n",
    "#             output_str += f\"{i} {xmin} {ymin} {xmax} {ymax}\\n\"\n",
    "\n",
    "# #             if i % 20 == 0:\n",
    "# #                 show_image(frame)\n",
    "        \n",
    "#         print(output_str)\n",
    "\n",
    "#         # When everything done, release the video capture object\n",
    "#         cap.release()\n",
    "\n",
    "#         # Close all the frames\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "#     #     # Reconstruct pred file name\n",
    "#     #     pred_file = video_names[j].split(\".\")[0] + \"_predicted.txt\"\n",
    "\n",
    "#     #     # Save output string to file\n",
    "#     #     with open(task2_dir_result + pred_file, 'w') as f:\n",
    "#     #             f.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2220979",
   "metadata": {},
   "source": [
    "## Second approach - Using tracker & YOLOv7 to recalibrate tracker\n",
    "The next approach is a bit more computationally expensive, but also more robust. It's still not perfect, as I'll mention in a second. The main idea here was to still use the tracker to track the car/vehicle, but once every few frames, make a YOLO prediction on all cars and make sure our bounding box is still covering our desired car. We do this by re-initializing our tracker with a given YOLO prediction, every few frames. \n",
    "\n",
    "This covers the problem of cars going too fast or too slow for our tracker, whilst also providing a more robust bounding box. By robust, I mean a bounding box that really covers the entire car. The tracker would ocasionally have too small or large bounding boxes for the car it was trying to track.\n",
    "\n",
    "On the other hand, it still fails to account for obstacles in front of the camera. If a pillar or a sempahore gets in the way of the camera, then even YOLO fails at predicting that car in the frames where view is obstructed. As such, the tracker can be only as good as YOLO will allow it. YOLO is still a good algorithm, but not perfect. As such, it will still fail on a some videos after a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043239f",
   "metadata": {},
   "source": [
    "### Actual processing of video\n",
    "Processing each video takes around 5 minutes per video, so entire cell should take around 1h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0af960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.mp4', '02.mp4', '03.mp4', '04.mp4', '05.mp4']\n",
      "['01.txt', '02.txt', '03.txt', '04.txt', '05.txt']\n",
      "884\n",
      "01.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 883/884 [05:51<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n",
      "02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 652/653 [04:19<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n",
      "03.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 884/885 [05:56<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935\n",
      "04.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 934/935 [06:09<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056\n",
      "05.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1055/1056 [06:53<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\n",
      "06.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1445/1446 [09:36<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n",
      "07.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 588/589 [03:54<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "08.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 347/348 [02:26<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1513\n",
      "09.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1512/1513 [09:56<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520\n",
      "10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1519/1520 [10:05<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678\n",
      "11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 677/678 [04:29<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n",
      "12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 460/461 [03:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 510/511 [03:23<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n",
      "14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 408/409 [02:42<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n",
      "15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 491/492 [03:17<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 22min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = f'{mode}/Task2/'\n",
    "# gt_path = path + \"ground-truth/\"\n",
    "\n",
    "# Store Video names\n",
    "video_names = read_mp4_files_in_directory(path)\n",
    "print(video_names[:5])\n",
    "\n",
    "# Get names of text files containing starting information\n",
    "task2_txt = [f\"{video_name.split('.')[0]}.txt\" for video_name in video_names]\n",
    "print(task2_txt[:5])\n",
    "\n",
    "# Iterate through all the video names\n",
    "for j, video_name in enumerate(video_names):\n",
    "    \n",
    "    # Get text file name given video name\n",
    "    txt_file = video_name.split(\".\")[0] + \".txt\"\n",
    "    \n",
    "    # Path to text file\n",
    "    file_path = path + txt_file\n",
    "\n",
    "    # Read in the 2 lists\n",
    "    file_data = read_file_structure_task2(file_path)\n",
    "    \n",
    "    # Get number of frames and bounding box\n",
    "    no_frames, rect_init = get_frames_and_rect(file_data)\n",
    "    print(no_frames)\n",
    "\n",
    "    # Create Output String\n",
    "    output_str = f\"{no_frames} -1 -1 -1 -1\\n\" \\\n",
    "                 f\"0 {rect_init[0]} {rect_init[1]} {rect_init[2]} {rect_init[3]}\\n\"\n",
    "   \n",
    "    # Path to video file\n",
    "    video_path = path + video_name\n",
    "    print(f\"{video_name}\\n\")\n",
    "\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened(): \n",
    "        print(\"Error opening video stream or file\")\n",
    "        exit()\n",
    "        \n",
    "    # Create a tracker\n",
    "    tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    "    # Get first frame\n",
    "    ret, frame_init = cap.read()\n",
    "\n",
    "    cv2.rectangle(\n",
    "        frame_init, \n",
    "        (rect_init[0], rect_init[1]), \n",
    "        (rect_init[0] + rect_init[2], rect_init[1] + rect_init[3]), \n",
    "        (0, 255, 0), \n",
    "        2\n",
    "    )\n",
    "\n",
    "    # DEBUG: check frame\n",
    "#     show_image(frame_init)\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    ok = tracker.init(frame_init, rect_init)\n",
    "        \n",
    "    # Frame counter\n",
    "    frame_cnt = 0\n",
    "    \n",
    "    # Iterate through all the frames in the video\n",
    "    for _ in tqdm(range(no_frames)):\n",
    "        \n",
    "        # Read current frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Update tracker every every frame\n",
    "        ok, bbox = tracker.update(frame)\n",
    "        \n",
    "        # Every 3 frames, run YOLO and update currently tracked objects\n",
    "        if frame_cnt % 3 == 0:\n",
    "            \n",
    "            # Use YOLO to detect cars in the current frame\n",
    "            objects_bbox = identify_vehicles_in_image(frame)\n",
    "            \n",
    "            # Try to see if any of the yolo predicted boxes is close to an existing tracked box\n",
    "            iou, candidate = get_max_iou_from_candidates(bbox, objects_bbox)\n",
    "            \n",
    "            # If we have a match, update current tracker\n",
    "            if iou > 0.1 and candidate:\n",
    "            \n",
    "                # You want to re-init the trackers with YOLO predictions\n",
    "                tracker.init(frame, candidate)\n",
    "                \n",
    "                # Update bbox value with candidate\n",
    "                bbox = candidate\n",
    "                 \n",
    "        # Create coordinates from rectangle and append to output str\n",
    "        xmin, ymin, xmax, ymax = get_rect_return_coord(*bbox)\n",
    "        frame_cnt += 1\n",
    "        \n",
    "        # Append output_str\n",
    "        output_str += f\"{frame_cnt} {xmin} {ymin} {xmax} {ymax}\\n\"\n",
    "       \n",
    "        # DEBUG\n",
    "#         if frame_cnt % 20 == 0:\n",
    "#             p1 = (int(bbox[0]), int(bbox[1]))\n",
    "#             p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "#             cv2.rectangle(frame, p1, p2, (0,255, 0), 2, 1)\n",
    "#             show_image(frame)     \n",
    "    \n",
    "    # Reconstruct pred file name\n",
    "    pred_file = video_names[j].split(\".\")[0] + \"_predicted.txt\"\n",
    "\n",
    "    # Save output string to file\n",
    "    with open(task2_dir_result + pred_file, 'w') as f:\n",
    "            f.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58e8ad",
   "metadata": {},
   "source": [
    "# --- END OF TASK 2 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea0a0e",
   "metadata": {},
   "source": [
    "#  TASK 3 Approach\n",
    "\n",
    "So actually, the idea of task 2 is simply expanded to task 3. I'll admit, I actually came with the idea of \"synchronising\" tracker and `YOLO` predictions in task 3, and then thought of trying it for task 2 as well. It's essential for us to know we can (for the most part) reliably track one car. Given we know how to track a single car, tracking more cars should just imply using more trackers.\n",
    "\n",
    "As such, each vehicle that appears in our video will have its own tracker. We will sync our tracker predcictions with what YOLO is able to detect. We will 'interlink' the predictions, to insure we have the highest rate of success. There are still edge cases where our code fails, which are similar to the ones present in Task2 (i.e. car being obstructed).\n",
    "\n",
    "But one question still remains - How do we know that a car traversed from, say, region 1 to region 2?\n",
    "\n",
    "We'll actually borrow part of the approach from Task 1. Instead of masking just one lane, we'll just mask regions (1, 2, 3). Like in Task 1, we'll predict, given a frame, where the cars are. We'll perform an overlap computation (IoU), to see which cars are in which regions, for a given frame. So for each frame, we know where there are cars, and if they're on a certain region.\n",
    "\n",
    "In order to put it all together, I actually instantiate some dictionaries (hashmaps) that track the path of a tracker (vehicle):\n",
    "- A dictionary for storing the tracker and its current location (bounding box) in the video\n",
    "- A dictionary where we store where we have first detected a tracker (source location)\n",
    "- A dictionary where we store the tracker and its path throughout the video (i.e. 2, None, 3) -> we know it was first seen in region 2, it passed the intersection (marked None), and then went in region 3\n",
    "\n",
    "At the end of the video, we'll check our paths, our sources, and increment the counts of cars that passed from one region to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986b0b6",
   "metadata": {},
   "source": [
    "### Actual processing of video\n",
    "\n",
    "Processing of a video depends on the GPU, but may take around 10-15 minutes per video. This cell will finish running in 3h, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83cfea1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.mp4', '02.mp4', '03.mp4', '04.mp4', '05.mp4']\n",
      "01.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [17:08<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 0\n",
      "2-1 0\n",
      "2-3 3\n",
      "3-1 0\n",
      "3-2 0\n",
      "02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2123/2123 [14:54<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 3\n",
      "2-1 2\n",
      "2-3 1\n",
      "3-1 0\n",
      "3-2 2\n",
      "03.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1748/1748 [12:34<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 1\n",
      "2-1 0\n",
      "2-3 1\n",
      "3-1 5\n",
      "3-2 2\n",
      "04.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [12:39<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 6\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 7\n",
      "3-2 0\n",
      "05.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2058/2058 [12:19<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 3\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 1\n",
      "3-2 1\n",
      "06.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [11:36<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 1\n",
      "2-1 1\n",
      "2-3 1\n",
      "3-1 1\n",
      "3-2 1\n",
      "07.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2351/2351 [13:04<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 0\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 0\n",
      "3-2 1\n",
      "08.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1832/1832 [10:04<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 0\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 0\n",
      "3-2 0\n",
      "09.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [13:58<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 1\n",
      "1-3 3\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 3\n",
      "3-2 2\n",
      "10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653/1653 [16:22<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 0\n",
      "2-1 0\n",
      "2-3 5\n",
      "3-1 0\n",
      "3-2 0\n",
      "11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1693/1693 [11:04<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 1\n",
      "1-3 0\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 4\n",
      "3-2 0\n",
      "12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2006/2006 [11:09<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 2\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 4\n",
      "3-2 1\n",
      "13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1782/1782 [15:17<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 4\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 3\n",
      "3-2 2\n",
      "14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [12:11<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 4\n",
      "2-1 0\n",
      "2-3 0\n",
      "3-1 3\n",
      "3-2 1\n",
      "15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1269/1269 [08:42<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2 0\n",
      "1-3 1\n",
      "2-1 1\n",
      "2-3 0\n",
      "3-1 0\n",
      "3-2 0\n",
      "Wall time: 3h 13min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = f'{mode}/Task3/'\n",
    "# gt_path = path + \"ground-truth/\"\n",
    "\n",
    "# Store Video names\n",
    "video_names = read_mp4_files_in_directory(path)\n",
    "print(video_names[:5])\n",
    "\n",
    "# task3_txt = [f\"{video_name.split('.')[0]}.txt\" for video_name in video_names]\n",
    "# print(task3_txt[:5])\n",
    "\n",
    "for j, video_name in enumerate(video_names):\n",
    "     # Path to your video file\n",
    "    video_path = path + video_name\n",
    "    print(f\"{video_name}\\n\")\n",
    "\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened(): \n",
    "        print(\"Error opening video stream or file\")\n",
    "        exit()\n",
    "    \n",
    "    # Count number of frames\n",
    "    no_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    trackers = {}  # {tracker: bounding box}\n",
    "    tracker_source = {}  # {tracker: first_seen_in_region (int)}\n",
    "    tracker_path = {} # {tracker: list (regions)}\n",
    "    \n",
    "    # Init frame count\n",
    "    frame_cnt = 0\n",
    "    \n",
    "    for _ in tqdm(range(no_frames)):\n",
    "        \n",
    "        # Read current frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Update trackers every 2 frames for efficiency\n",
    "        if frame_cnt % 2 == 0:\n",
    "            for tracker in trackers:\n",
    "                tracker.update(frame)\n",
    "        \n",
    "        # Every 4 frames, run YOLO and update currently tracked objects\n",
    "        if frame_cnt % 4 == 0:\n",
    "            \n",
    "            # Use YOLO to detect cars in the current frame every few frames\n",
    "            new_boxes = identify_vehicles_in_image(frame)\n",
    "            \n",
    "            new_trackers = {}\n",
    "            \n",
    "            for new_box in new_boxes:\n",
    "                \n",
    "                # Try to see if predicted box is close to an existing tracked box\n",
    "                iou, candidate = get_max_iou_from_candidates(new_box, trackers.values())\n",
    "                \n",
    "                # If we have a match, update current tracker\n",
    "                if iou > 0.02 and candidate:\n",
    "                    \n",
    "                    # Update the tracker bounding box as YOLO's preds are more robust\n",
    "                    tracker = get_key(trackers, candidate)\n",
    "                    \n",
    "                    # You want to re-init the trackers with YOLO predictions\n",
    "                    tracker.init(frame, new_box)\n",
    "                    \n",
    "                    # Store new-Trackers to replace trackers at end of prediction\n",
    "                    new_trackers[tracker] = new_box\n",
    "                    \n",
    "                    # Check location of current bounding box (as source)\n",
    "                    current_region = check_region_task3_bounding_box(frame, new_box)\n",
    "                    \n",
    "                    # Check if object has changed region and add to path if so\n",
    "                    if tracker_path[tracker][-1] != current_region:\n",
    "                        tracker_path[tracker].append(current_region)\n",
    "                    \n",
    "                else:\n",
    "                    # The object is new, added to tracked objects\n",
    "                    tracker = cv2.TrackerCSRT_create()\n",
    "                    \n",
    "                    # Initialize tracker with first frame and bounding box\n",
    "                    ok = tracker.init(frame, new_box)\n",
    "                    \n",
    "                    # Store the tracker in our trackers dict\n",
    "                    new_trackers[tracker] = new_box\n",
    "                    \n",
    "                    # Check location of current bounding box (as source)\n",
    "                    source_region = check_region_task3_bounding_box(frame, new_box)\n",
    "                    \n",
    "                    # Update tracker_source\n",
    "                    tracker_source[tracker] = source_region\n",
    "                    \n",
    "                    # Update tracker_current\n",
    "                    tracker_path[tracker] = [source_region]\n",
    "                \n",
    "        # Rremove trackers that did not match predicted YOLO boxes (by replacing old trackers with new one)\n",
    "        trackers = new_trackers    \n",
    "        \n",
    "        # DEBUG\n",
    "#         if frame_cnt % 20 == 0:\n",
    "#             # Used for debugging the trackers\n",
    "#             for bbox in trackers.values():\n",
    "#                 p1 = (int(bbox[0]), int(bbox[1]))\n",
    "#                 p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "#                 cv2.rectangle(frame, p1, p2, (0,255,0), 2, 1)\n",
    "#             show_image(frame)\n",
    "        \n",
    "        frame_cnt += 1\n",
    "        \n",
    "    tracker_path = process_tracker_path(tracker_path)\n",
    "#     print(tracker_path)\n",
    "    output_str = compute_score_tracker_path(tracker_path)\n",
    "    \n",
    "    # Reconstruct pred file name\n",
    "    pred_file = video_names[j].split(\".\")[0] + \"_predicted.txt\"\n",
    "\n",
    "    # Save output string to file\n",
    "    with open(task3_dir_result + pred_file, 'w') as f:\n",
    "            f.write(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
